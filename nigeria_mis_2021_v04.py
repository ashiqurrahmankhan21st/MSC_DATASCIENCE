# -*- coding: utf-8 -*-
"""Nigeria MIS 2021_v04.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UgSlRSXVw5ySDvU0rI7uv3Bl5UzD7YyH
"""

import pandas as pd
import seaborn as sns
import numpy as np
from sklearn.impute import KNNImputer
import time

"""# 'NGPR81FL.DTA'"""

#ds = ['NGFW81FL.DTA','NGHR81FL.DTA','NGIR81FL.DTA','NGKR81FL.DTA','NGPR81FL.DTA']
#df = pd.read_stata(ds[4],convert_categoricals=False)
#df = pd.read_stata('NGPR81FL.DTA',convert_categoricals=False)
#df.to_csv('DataMalaria.csv')
#df.head()

#df = pd.read_csv('NGPR81FL.csv',low_memory=False)
df = pd.read_csv('/content/drive/MyDrive/DataScience/DataMalaria.csv',low_memory=False)
dfraw = df.copy()
dfraw.head()

features = pd.read_csv('/content/drive/MyDrive/DataScience/features.csv')
for i in range(len(features)):
    if features.iloc[i,3] == "categorical":
        features.iloc[i,3]  = "category"
    elif features.iloc[i,3] == "int":
        features.iloc[i,3]  = "int64"
    else:
        features.iloc[i,3]  = "float64"

features

# hml32 = Final result of malaria from blood smear test[0-Negative,1-Positive], 
# hml35 = Result of malaria rapid test [0-Negative,1-Positive]

dfn = dfraw[features["var"]].copy()
dfn.head()

# hc1a = Child's age in days

dfn['hc1a'] = dfraw['hc1a'] # assigning the child column to dataset

# hml32 = Final result of malaria from blood smear test[0-Negative,1-Positive],

dfNe = dfn[(dfn['hml32'] == 0) | (dfn['hml32'] == 1)].copy()
dfNe = dfNe.reset_index()
del dfNe['index']
me = dfNe.copy()
dfNe.head()

dfNe.shape

#dfNe = dfNe.astype("category")

dfNe.info()

features['counts']=0
for i in range(len(dfNe.columns)):
    features.iloc[i,-1] = (dfNe.iloc[:,i].value_counts()).sum()
features

dfNe = dfNe.drop(columns = ['sh130']) # dropping due to no data
dfNe = dfNe.drop(columns = ['hv202']) # Source of non-drinking water
dfNe = dfNe.drop(columns = ['hml23']) # Place where net was obtained

dff = dfNe.copy()

"""# Reserving data for KNN Imputation"""

DataFarmeKNNImputation = dfNe.copy()

DataFarmeKNNImputation.head()

DataFarmeKNNImputation.shape

"""# Filtering start"""

flist = []
count = []
for i in range(len(dff.columns)):
    flist.append(dff.columns[i])
    count.append((dff.iloc[:,i].value_counts()).sum())
nf = pd.DataFrame({
    'Features':flist,
    'count' : count
})
nf

nf['count'].min()

dff['hv225'].value_counts()

#1
c = dff[(dff['hml37l'] == 0) | (dff['hml37l'] == 1)]

flist = []
count = []
for i in range(len(c.columns)):
    flist.append(c.columns[i])
    count.append((c.iloc[:,i].value_counts()).sum())
nf = pd.DataFrame({
    'Features':flist,
    'count' : count
})
nf

nf['count'].min()

c['hml22'].value_counts()

#2
c = c[(c['hml22'] == 0) | (c['hml22'] == 1) | (c['hml22'] == 2) | 
       (c['hml22'] == 3)]

c

flist = []
count = []
for i in range(len(c.columns)):
    flist.append(c.columns[i])
    count.append((c.iloc[:,i].value_counts()).sum())
nf = pd.DataFrame({
    'Features':flist,
    'count' : count
})
nf

nf['count'].min()

c['hv225'].value_counts()

#3
c = c[(c['hv225'] == 0) | (c['hv225'] == 1)]

c

flist = []
count = []
for i in range(len(c.columns)):
    flist.append(c.columns[i])
    count.append((c.iloc[:,i].value_counts()).sum())
nf = pd.DataFrame({
    'Features':flist,
    'count' : count
})
nf

nf['count'].min()

c['hc61'].value_counts()

#4
c = c[(c['hc61'] == 0) | (c['hc61'] == 1) | (c['hc61'] == 2) | 
       (c['hc61'] == 3)]

c

flist = []
count = []
for i in range(len(c.columns)):
    flist.append(c.columns[i])
    count.append((c.iloc[:,i].value_counts()).sum())
nf = pd.DataFrame({
    'Features':flist,
    'count' : count
})
nf

nf['count'].min()

c['hv235'].value_counts()

#5
c = c[(c['hv235'] == 1) | (c['hv235'] == 2) | (c['hv235'] == 3)]

c

flist = []
count = []
for i in range(len(c.columns)):
    flist.append(c.columns[i])
    count.append((c.iloc[:,i].value_counts()).sum())
nf = pd.DataFrame({
    'Features':flist,
    'count' : count
})
nf

nf['count'].min(),nf['count'].max()

c.dropna()

#c.to_csv("finalData23features.csv")

dff = c.copy()

#*****************************************************

"""# need a version 2 ipython

# total Case of Malaria Positive and Negative in Final Test result
"""

dff['hml32'].value_counts()

ddf = dff.copy()
ddf

ddf.dropna()

ddf = ddf.reset_index()
del ddf['index']
ddf = ddf.copy()
ddf

#**************************************

features

"""#flist = []
#for i in range(len(ddf.columns)):
#    for j in range(len(features)):
#        if ddf.columns[i] == features.iloc[j,0]:
#            flist.append(features.iloc[j,2])
#NewFeatures = pd.DataFrame(flist)
#NewFeatures
"""

ddf.columns

NewFeatures = pd.read_csv("/content/drive/MyDrive/DataScience/Newfeatures.csv")
NewFeatures

ddf.columns == NewFeatures['var']

NewFeatures['DataType'] = NewFeatures['DataType'].replace('categorical','category')

NewFeatures

"""# Converting the DataType"""

for i in range(len(ddf.columns)):
    dtype =  NewFeatures.iloc[i,3]
    ddf.iloc[:,i] = ddf.iloc[:,i].astype(dtype)

ddf.info()

temp = ddf.copy()

temp.columns = NewFeatures["Description"]

temp.columns

temp.to_csv("DataForFurtherAnalysis.csv")

temp.info()



dfROS = ddf.copy()
dfSMOTE = ddf.copy()

sns.countplot(x = ddf["hml35"])
ddf['hml35'].value_counts()

sns.countplot(x = ddf.iloc[:,2])

NewFeatures

ele = ['Has mosquito bed net for sleeping', "Presence of species: vivax (Pv)","Result of malaria rapid test" ]

#ele = ['hv227','hml32d','hml35']

del temp[ele[0]]
del temp[ele[1]]
del temp[ele[2]]

temp.columns

temp[temp['Time to get to water source (minutes)'] != 996]#hv204

#temp[temp['hv204'] != 996]

temp.info()

"""# Chi_Squared test and Kruskal test"""

from scipy import stats
from scipy.stats import chi2_contingency
from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()

def Chi_Squared(x,y):
    contigency= pd.crosstab(x,y)
    c, p, dof, expected = chi2_contingency(contigency)
    v = c,p,dof
    return v

def Kruskal(x,y):   
    #f = encoder.fit_transform(x)
    v = stats.kruskal(x,y)
    return v

def corrr(data):
    
    #data = ddf.copy()
    col = data.columns
    le = len(data.columns)
    cof = pd.DataFrame({
            "var": data.columns,
            "Chi": np.zeros(le),
            "p-value": np.zeros(le),
            "dof":np.zeros(le),
            'Dtype':np.zeros(le),
            "Decision":np.zeros(le)
        })

    
    varl = []
    
    for i in range(len(data.columns)):
        if data.iloc[:,i].dtype == "category":
            #data.iloc[:,i] = encoder.fit_transform(data.iloc[:,i])#.astype('category')
            cof.iloc[i,1],cof.iloc[i,2],cof.iloc[i,3] = Chi_Squared(data.iloc[:,i],data['Final result of malaria from blood smear test'])
            #'Final result of malaria from blood smear test' = 'hml32'
            cof.iloc[i,-2] = data.iloc[:,i].dtype
        else:
            v = Kruskal(data.iloc[:,i],data['Final result of malaria from blood smear test'])
            cof.iloc[i,1] = v[0]
            cof.iloc[i,2] = v[1]
            cof.iloc[i,3] = 1
            cof.iloc[i,-2] = data.iloc[:,i].dtype
        
    cof['Decision'] = cof["p-value"]<0.05
    for i in range(len(data.columns)):
        if cof.iloc[i,-1] != False:
            varl.append(cof.iloc[i,0])

    DATA = data[varl]
    
    
        
    return cof,DATA

corr = corrr(temp)

corr[0]

#for i in range(37):
#    print(i+1)

newdf = corr[1].copy()
newdf

newdf.columns

var = corrr(newdf)[0].copy()
var

newdf.shape

newdf['Final result of malaria from blood smear test'].value_counts()

"""# Over Sampling"""

# check version number
import imblearn
from imblearn.over_sampling import SMOTE
from imblearn.over_sampling import RandomOverSampler
print(imblearn.__version__)

"""# RandomOverSampler"""

n = 10000

n*.55

oversample = RandomOverSampler(sampling_strategy={0:int(n*0.55),1:int(n*0.45)},random_state=123)
Xx = (newdf.drop(columns=['Final result of malaria from blood smear test'])).copy()
Yy = newdf['Final result of malaria from blood smear test'].copy()
X_over, y_over = oversample.fit_resample(Xx, Yy)
sns.countplot(x=y_over)
print(y_over.value_counts())

DataFrameROS = pd.DataFrame(X_over)
DataFrameROS['Final result of malaria from blood smear test'] = y_over
DataFrameROS.head()

DataFrameROS.shape

#Logistic Regression (LR)
#Decision Tree (DT)/ Random Forest
#Automated Machine Learning (AutoML) framework H2O

import pandas as pd
from sklearn import svm
from sklearn import tree
from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
from sklearn.model_selection import train_test_split

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import RidgeClassifier

from sklearn.model_selection import cross_val_score
from sklearn import preprocessing
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import precision_recall_fscore_support
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
from sklearn import metrics

DataFrameROSX = DataFrameROS.drop(columns=["Final result of malaria from blood smear test"]).copy()
DataFrameROSY = DataFrameROS["Final result of malaria from blood smear test"].copy()

encoder = LabelEncoder()
DataFrameROSY = encoder.fit_transform(DataFrameROSY)

X_train, X_test, y_train, y_test = train_test_split(DataFrameROSX, DataFrameROSY, test_size=0.2, random_state=123)
X_test, X_val, y_test, y_val     = train_test_split(X_test, y_test, test_size=0.5, random_state=123)

sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test  = sc.fit_transform(X_test)
X_val   = sc.fit_transform(X_val)

met

def ML(model,X_train, X_test,X_val, y_train, y_test, y_val):

  met = pd.DataFrame({
    "mae": np.zeros(3),
    "rmsee":np.zeros(3),
    "r2":np.zeros(3),
    "precision":np.zeros(3),
    "recall":np.zeros(3),
    "f1score":np.zeros(3),
    "accuracy":np.zeros(3),
    }).T
  met.columns = ["Train","Test","Val"] ; met = met.T
 
  for i in range(2):
    if i == 0:
      y_pred = model.predict(X_train)
      y      = y_train
      X = X_train
    else:
      y_pred = model.predict(X_test)
      y      = y_test
      X = X_test

    met.iloc[i,0] = mean_absolute_error(y, y_pred)
    met.iloc[i,1] = mean_squared_error(y, y_pred, squared=False)
    met.iloc[i,2] = r2_score(y, y_pred)
    met.iloc[i,3] = metrics.precision_score(y, y_pred)
    met.iloc[i,4] = metrics.recall_score(y, y_pred)
    met.iloc[i,5] = metrics.f1_score(y, y_pred)
    met.iloc[i,6] = model.score(X, y)

  
  met.iloc[2,0] = abs(cross_val_score(model, X_val, y_val, cv=10, scoring='neg_mean_absolute_error').mean())
  met.iloc[2,1] = abs(cross_val_score(model, X_val, y_val, cv=10, scoring='neg_root_mean_squared_error').mean())
  met.iloc[2,2] = cross_val_score(model, X_val, y_val, cv=10, scoring='r2').mean()
  met.iloc[2,3] = cross_val_score(model, X_val, y_val, cv=10, scoring='precision').mean()
  met.iloc[2,4] = cross_val_score(model, X_val, y_val, cv=10, scoring='recall').mean()
  met.iloc[2,5] = cross_val_score(model, X_val, y_val, cv=10, scoring='f1').mean()
  met.iloc[2,6] = cross_val_score(model, X_val, y_val, cv=10, scoring='accuracy').mean()

  return met

LR = LogisticRegression(penalty="l2",
                        max_iter=1000,solver="newton-cholesky",
                        random_state=123).fit(X_train, y_train)
ML(LR,X_train, X_test,X_val, y_train, y_test, y_val)



DT = DecisionTreeClassifier(random_state=123,
                            max_depth=4,max_features='sqrt',
                            splitter = "random",
                            ccp_alpha = 0.01,
                            ).fit(X_train, y_train)
ML(DT,X_train, X_test,X_val, y_train, y_test, y_val)





import h2o
h2o.init()
from h2o.model.segment_models import H2OFrame
from h2o.estimators import H2ORandomForestEstimator
from h2o.automl import H2OAutoML

h2odf2 = DataFrameROS.copy()
df2h2o = H2OFrame(h2odf2)
df2h2o["Final result of malaria from blood smear test"] = df2h2o["Final result of malaria from blood smear test"].asfactor()
train, test, valid = df2h2o.split_frame(ratios=[.8, .1],seed=123)
y = "Final result of malaria from blood smear test"
x = df2h2o.columns
x.remove(y)

# Run AutoML for  base models
aaml = H2OAutoML(max_models=5, seed=123, nfolds = 10,sort_metric= 'AUC')
aaml.train(x=x, y=y, training_frame=train, validation_frame=valid)

lb = aaml.leaderboard
lb.head(rows=lb.nrows)

bbest_model = aaml.get_best_model()
print(bbest_model)

bbest_model.learning_curve_plot()

#bbest_model.varimp_plot()

print("train accuracy                 : ",best_model.accuracy(train=True, valid=False, xval=False)[0][1])
print("validation accuracy            : ",best_model.accuracy(train=False, valid=True, xval=False)[0][1])
print("after validation train accuracy: ",best_model.model_performance(train).accuracy()[0][1])
print("test accuracy                  : ",best_model.model_performance(test).accuracy()[0][1])
H2OTr = best_model.model_performance(train).accuracy()[0][1]
H2OTe = best_model.model_performance(test).accuracy()[0][1]

best_model.mse(train=False, valid=False, xval=True)#[0][1]



"""**Original** **Data**"""

DataFrameROSX = newdf.drop(columns=["Final result of malaria from blood smear test"]).copy()
DataFrameROSY = newdf["Final result of malaria from blood smear test"].copy()

encoder = LabelEncoder()
DataFrameROSY = encoder.fit_transform(DataFrameROSY)

X_train, X_test, y_train, y_test = train_test_split(DataFrameROSX, DataFrameROSY, test_size=0.2, random_state=123)
X_test, X_val, y_test, y_val     = train_test_split(X_test, y_test, test_size=0.5, random_state=123)

LR = LogisticRegression(penalty="l2",
                        max_iter=1000,solver="newton-cholesky",
                        random_state=123).fit(X_train, y_train)
ML(LR,X_train, X_test,X_val, y_train, y_test, y_val)

DT = DecisionTreeClassifier(random_state=123,
                            max_depth=4,max_features='sqrt',
                            splitter = "random",
                            ccp_alpha = 0.01,
                            ).fit(X_train, y_train)
ML(DT,X_train, X_test,X_val, y_train, y_test, y_val)

h2odf2 = newdf.copy()
df2h2o = H2OFrame(h2odf2)
df2h2o["Final result of malaria from blood smear test"] = df2h2o["Final result of malaria from blood smear test"].asfactor()
train, test, valid = df2h2o.split_frame(ratios=[.8, .1],seed=123)
y = "Final result of malaria from blood smear test"
x = df2h2o.columns
x.remove(y)

# Run AutoML for  base models
aml = H2OAutoML(max_models=5, seed=123, nfolds = 10,sort_metric= 'AUC')
aml.train(x=x, y=y, training_frame=train, validation_frame=valid)

best_model = aml.get_best_model()
print(best_model)

best_model.learning_curve_plot()

best_model.precision(train=True, valid=False, xval=False)[0][1]

best_model.varimp_plot()









s = H2OFrame(g)
s['hml32'] =s['hml32'].asfactor()
train, test, valid = s.split_frame(ratios=[.8, .1],seed=123)
y = 'hml32'
x = s.columns
x.remove(y)
model = H2ORandomForestEstimator(ntrees=130,
                                    max_depth=2,
                                    min_rows=13,
                                    calibrate_model=True,
                                    calibration_frame=valid,
                                    binomial_double_trees=True,
                                    distribution = 'auto',seed = 123)
model.train(x=x, y=y, training_frame=train, validation_frame=valid)
perf = model.model_performance()
print("train accuracy                 : ",model.accuracy(train=True, valid=False, xval=False)[0][1])
print("validation accuracy            : ",model.accuracy(train=False, valid=True, xval=False)[0][1])
print("after validation train accuracy: ",model.model_performance(train).accuracy()[0][1])
print("test accuracy                  : ",model.model_performance(test).accuracy()[0][1])
model.learning_curve_plot()
model.varimp_plot()
model.plot(metric="auc")

#dfh2['target'] = dfh2['target'].asfactor()
f = H2OFrame(me)
f['hml32'] =f['hml32'].asfactor()
train, test, valid = f.split_frame(ratios=[.8, .1],seed=123)
y = 'hml32'
x = f.columns
x.remove(y)

model = H2ORandomForestEstimator(ntrees=130,
                                    max_depth=2,
                                    min_rows=13,
                                    calibrate_model=True,
                                    calibration_frame=valid,
                                    binomial_double_trees=True,
                                    distribution = 'auto',seed = 123)
model.train(x=x, y=y, training_frame=train, validation_frame=valid)
perf = model.model_performance()
print("train accuracy                 : ",model.accuracy(train=True, valid=False, xval=False)[0][1])
print("validation accuracy            : ",model.accuracy(train=False, valid=True, xval=False)[0][1])
print("after validation train accuracy: ",model.model_performance(train).accuracy()[0][1])
print("test accuracy                  : ",model.model_performance(test).accuracy()[0][1])
model.learning_curve_plot()
model.varimp_plot()
model.plot(metric="auc")

model.plot(metric="auc")













LR = LogisticRegression(penalty="l2",
                        max_iter=10000,solver="newton-cholesky",
                        random_state=123).fit(X_train, y_train)

LTr = LR.score(X_train, y_train)
LTe = LR.score(X_test, y_test)
y_pred = LR.predict(X_train)
LMA = mean_absolute_error(y_train, y_pred)
LRM = mean_squared_error(y_train, y_pred, squared=False)
LR2 = r2_score(y_train, y_pred)

print("train MAE",LMA)
print("train RMSE",LRM)
print("train R2",LR2)
print("train accuracy: ",LTr)
print("test accuracy: ",LTe)
print('Precision',metrics.precision_score(y_train, y_pred))
print('Recall',metrics.recall_score(y_train, y_pred))
print('f1Score',metrics.f1_score(y_train, y_pred))

DT = DecisionTreeClassifier(random_state=123).fit(X_train, y_train)
DTr = DT.score(X_train, y_train)
DTe = DT.score(X_test, y_test)
print("train accuracy: ",DTr)
print("test accuracy: ",DTe)
path = DT.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas, impurities = path.ccp_alphas, path.impurities
ccp_alphas = (ccp_alphas.round(5))*100
impurities = (impurities.round(5))*100
fig, ax = plt.subplots()
ax.plot(ccp_alphas[:-1],impurities[:-1],marker = 'o',drawstyle = "steps-post")
ax.set_xlabel("alpha")
ax.set_ylabel("impurity")
tree.plot_tree(DT)
plt.show()



DT = DecisionTreeClassifier(random_state=123,
                            max_depth=4,max_features='sqrt',
                            splitter = "random",
                            ccp_alpha = 0.05,
                            ).fit(X_train, y_train)
DTr = DT.score(X_train, y_train)
DTe = DT.score(X_test, y_test)
print("train accuracy: ",DTr)
print("test accuracy: ",DTe)
tree.plot_tree(DT)
plt.show()

